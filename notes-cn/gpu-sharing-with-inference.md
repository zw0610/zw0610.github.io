# GPU 共享 Special：模型服务

在之前的讨论中，我们有一个比较基本的结论：GPU 共享比较适合用在模型服务中。

那么实际操作的时候，怎么做会比较何时呢？这里我想和大家一起探讨这个问题。

## 模型服务的 Scope

模型服务作为整个机器学习 Pipeline 中的一个环节， 那么它的 scope 要如何限定呢？

## 回顾 GPU 共享方案

1. MPS
2. Multi-Streams
3. MIG

## 模型服务架构

![model-inference-arch](./gpu-sharing-with-inference/arch.svg)

## 服务启动流程

## 组建选型

## 其他的考虑